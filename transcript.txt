
          [MOUSE CLICKING] [MUSIC PLAYING] - This town ain't seen nothing yet. [NEW RADICALS, "YOU GET WHAT YOU GIVE"] [APPLAUSE, CHEERING] SUNDAR PICHAI: Wow, cool video. Looks like the team has been having some fun. Hello, everyone. Good morning. Welcome to Google I/O. [APPLAUSE, CHEERING] So good to see everyone here in Shoreline. And hello to everyone joining virtually around the world. I learned that today is the start of Gemini season. I'm not really sure what the big deal is. Every day is Gemini season here at Google.
          
          Normally, you wouldn't have heard much from us in the weeks leading up to I/O. That's because we'd be saving our best models for this stage. But in our Gemini era, we are just as likely to ship our most intelligent model on a random Tuesday in March or a really cool breakthrough like AlphaEvolve just a week before. We want to get our best models into your hands and our products ASAP. And so we are shipping faster than ever. We have announced over a dozen models and research breakthroughs and released over 20 major AI
          
          products and features, all since the last I/O. I'm particularly excited about the rapid model progress. You can see the step function change here. Elo scores, one measure of progress, are up more than 300 points since the first generation of Gemini Pro. And today, Gemini 2.5 Pro sweeps the LMArena leaderboard in all categories. [APPLAUSE, CHEERING] It is state of the art on many, many benchmarks. And we have also made rapid progress in coding. Our updated 2.5 Pro reached number one on WebDev Arena,
          
          and now surpasses the previous version by 140 Elo points. It's getting a lot of love across the top coding platforms, thanks to all of you. On Cursor, the leading AI code editor, Gemini is the fastest growing model of the year, producing hundreds of thousands of lines of accepted code every single minute. Now, this last milestone might be the most impressive, at least in some circles. A couple of weeks ago, Gemini completed "Pok√©mon Blue." [APPLAUSE, CHEERING] It earned all eight badges, traveled to Victory Road,
          
          defeated the Elite Four, and the champion, bringing us one step closer to achieving API, Artificial Pokemon Intelligence. [APPLAUSE, CHEERING] All of this progress is enabled by our world-leading infrastructure, the foundation of our full stack approach to AI. Our seventh generation TPU, Ironwood, is the first design to power thinking and inference at scale. It delivers 10x the performance over the previous generation and packs an incredible 42.5 exaflops of compute per pod. Just amazing. And it's coming to Google Cloud customers later this year.
          
          [APPLAUSE] Our infrastructure strength down to the TPU is what helps us deliver dramatically faster models. Of the top models on the LMArena leaderboard, Gemini holds the top three spots for highest output tokens generated per second, all while model prices are coming down significantly. There's a hard trade off between price and performance, yet over and over, we've been able to deliver the best models at the most effective price point. Not only is Google leading this parade of frontier, we have fundamentally shifted the frontier itself.
          
          The result, more intelligence available for everyone, everywhere. And the world is responding and adopting AI faster than ever before. It's one marker of progress. This time last year, we were processing 9.7 trillion tokens a month across our products and APIs. Now we are processing 480 trillion monthly tokens. That's about a 50x increase in just a year. [APPLAUSE] We're also seeing a wave of adoption across our developer AI tools. Today, over 7 million developers have built with the Gemini API
          
          across both Google AI Studio and Vertex AI, over 5x growth since last I/O. And Gemini usage on Vertex AI is up more than 40 times since last year. AI adoption is increasing across our products. The Gemini app now has over 400 million monthly active users. We are seeing strong growth and engagement, particularly with 2.5 models. For those using 2.5 Pro in the Gemini app, usage has gone up 45%. You will hear a lot more about the Gemini app later. We are also seeing incredible momentum in Search. Today, AI Overviews have more than 1.5 billion users
          
          every month. That means Google Search is bringing generative AI to more people than any other product in the world. And along with AI Overviews, AI Mode is the next big step for search. You'll hear more about this later. What all this progress means is that we are in a new phase of the AI platform shift, where decades of research are becoming reality for people all over the world. I want to share three examples of how research is transforming our products today-- projects Starline, Astra, and Mariner.
          
          We debuted project Starline, our breakthrough 3D video technology, at I/O a few years back. The goal was to create a feeling of being in the same room as someone, even if you were far apart. We've continued to make technical advances, and today we are ready to announce our next chapter. Introducing Google Beam, a new AI first video communications platform. Beam uses a new state of the art video model to transform 2D video streams into a realistic 3D experience. Behind the scenes, an array of six cameras
          
          captures you from different angles. And with AI, we can merge these video streams together and render you on a 3D lightfield display, with near perfect head tracking down to the millimeter and at 60 frames per second, all in real time. The result, a much more natural and deeply immersive conversational experience. We are so excited to bring this technology to others. In collaboration with HP, the first Google Beam devices will be available for early customers later this year. HP will have a lot more to share a few weeks from now.
          
          Stay tuned. [APPLAUSE, CHEERING] Over the years, we've been bringing underlying technology from Starline into Google Meet. That includes real-time speech translation to help break down language barriers. Here's an example of how this could be useful when booking a vacation rental in South America and you don't speak the language. Let's take a look. [VIDEO PLAYBACK] - Hi, Camilla. Let me turn on speech translation. It's nice to finally talk to you. - [NON-ENGLISH SPEECH] - [NON-ENGLISH SPEECH] - You're going to have a lot of fun,
          
          and I think you're going to love visiting the city. The house is in a very nice neighborhood and overlooks the mountains. - That sounds wonderful. Is the house-- - [NON-ENGLISH SPEECH] - [NON-ENGLISH SPEECH] - There's a bus nearby, but I would recommend renting a car so you can visit the nature and enjoy it. - That sounds great [NON-ENGLISH SPEECH] [END PLAYBACK] [APPLAUSE] SUNDAR PICHAI: You can see how well it matches the speaker's tone, patterns, and even their expressions. We are even closer to having a natural and free
          
          flowing conversation across languages. And today, we are introducing this real-time speech translation directly in Google Meet. English and Spanish translation is now available for subscribers, with more languages rolling out in the next few weeks. And real-time translation will be coming to Enterprises later this year. [APPLAUSE] Another early research project that debuted on the I/O stage was Project Astro. It explores the future capabilities of a universal AI assistant that can understand the world around you.
          
          We are starting to bring it to our products. Today, Gemini Live has project Astro's camera and screen sharing capabilities so you can talk about anything you see. People are using it in so many ways, whether practicing for a job interview or training for a marathon. We've been appreciating the feedback from our trusted testers and some who are a little less trusted. Take a look. [VIDEO PLAYBACK] - That's a pretty nice convertible. - I think you might have mistaken the garbage truck for a convertible.
          
          Is there anything else I can help you with? - What's this skinny building doing in my neighborhood? - It's a street light, not a building. - Why are these palm trees so short? I'm worried about them. - They're not short. They're actually pretty tall. - Sick convertible. - Garbage truck again. Anything else? - Why do people keep delivering packages to my lawn? - It's not a package. It's a utility box. - Why is this person following me wherever I walk? - No one's following you. That's just your shadow.
          
          [END PLAYBACK] [APPLAUSE, CHEERING] SUNDAR PICHAI: Gemini is pretty good at telling you when you're wrong. We are rolling this out to everyone on Android and iOS starting today. [APPLAUSE, CHEERING] Next, we also have our research prototype Project Mariner. It's an agent that can interact with the web and get stuff done. Stepping back, we think of agents as systems that combine the intelligence of advanced AI models with access to tools. They can take actions on your behalf and under your control.
          
          Computer use is an important agentic capability. It's what enables agents to interact with and operate browsers and other software. Project Mariner was an early step forward in testing computer-use capabilities. We released it as an early research prototype in December, and we have made a lot of progress since. First, we are introducing multitasking, and it can now oversee up to 10 simultaneous tasks. Second, it's using a feature called Teach and repeat. This is where you can show it a task once
          
          and it learns a plan for similar tasks in the future. We are bringing project Mariner's computer-use capabilities to developers via the Gemini API. Trusted testers, like Automation Anywhere and UiPath, are already starting to build with it, and it will be available more broadly this summer. Computer-use is part of a broader set of tools we will need to build for an agent ecosystem to flourish, like an open Agent2Agent protocols so that agents can talk to each other. We launched this at Cloud Next, with the support
          
          of over 60 technology partners, and hope to see that number grow. Then there is the model context protocol introduced by Anthropic so agents can access other services. And today, we are excited to announce that our Gemini SDK is now compatible with MCP tools. [APPLAUSE] These technologies will work together to make agents even more useful. And we are starting to bring agentic capabilities to Chrome, Search, and the Gemini app. Let me show you what we are excited about in the Gemini app. We call it Agent Mode.
          
          Say you want to find an apartment for you and two roommates in Austin. You've each got a budget of $1,200 a month. You want a washer/dryer, or at least a laundromat nearby. Normally, you'd have to spend a lot of time scrolling through endless listings. Using Agent Mode, the Gemini app goes to work behind the scenes. It finds listings from sites like Zillow that match your criteria and uses Project Mariner, when needed, to adjust very specific filters. If there's an apartment you want to check out,
          
          Gemini uses MCP to access the listings and even schedule a tour on your behalf. And it'll keep browsing for new listings for as long as you need, freeing up to do the stuff you want to do, like plan the housewarming party. It's great for companies like Zillow bringing in new customers and improving conversion rates. An experimental version of the Agent Mode in the Gemini app will be coming soon to subscribers. This is a new and emerging area, and we are excited to explore how best to bring the benefits of agents to users and the ecosystem more broadly.
          
          The best way we can bring research into reality is to make it really useful in your own reality. That's where personalization will be really powerful. We are working to bring this to life with something we call personal context. With your permission, Gemini models can use relevant context across your Google apps in a way that is private, transparent, and fully under your control. Let me show you an example in Gmail. You might be familiar with our AI-powered Smart Reply features. It's amazing how popular they are.
          
          Now imagine if those responses could sound like you. That's the idea behind Personalized Smart Replies. Let's say my friend wrote to me looking for advice. He's taking a road trip to Utah, and he remembers I did this trip before. Now, if I'm being honest, I would probably reply something short and unhelpful. Sorry, Felix. But with Personalized Smart Replies, I can be a better friend. That's because Gemini can do almost all the work for me, looking up my notes and drive, scanning past emails for reservations, and finding
          
          my itinerary in Google Docs, trip to Zion National Park. Gemini matches my typical greetings from past emails, captures my tone, style, and favorite word choices, and then it automatically generates a reply. I love how it included details like keeping driving time under five hours per day, and it uses my favorite adjective, exciting. Looks great! Maybe you want to make a couple of changes to it and hit Send. This will be available in Gmail this summer for subscribers. [APPLAUSE, CHEERING] And you can imagine how helpful personal contacts
          
          will be across Search, Docs, Gemini, and more. Today, I've talked about intelligence, agents, and personalization. These are a few of the frontiers where we will make progress. And you'll hear more examples throughout the keynote. But first, I want to invite someone who can share more about the intelligence driving our future innovation. Last year, I introduced him as Sir Demis. This year, we can add Nobel laureate to his list of titles. Come on out, Demis. [MUSIC PLAYING] DEMIS HASSABIS: Hey, everyone.